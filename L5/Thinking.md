Thinking1	什么是矩阵分解，都有哪些应用场景？	




Thinking2	矩阵分解算法ALS-WR是如何进行的？

	
Thinking3	梯度下降法中的批量梯度下降（BGD），随机梯度下降（SGD），和小批量梯度下降有什么区别（MBGD）	
BGD每次更新会将所有样本重新计算，稳定精确的趋向最优解，但是计算效率低；
SGD方法每次随机选用一个样本代替所有样本进行迭代计算，好处是大大节省了计算量，收敛速度很快，SGD与BGD的收敛方式略有不同，BGD是稳定的步进式趋于最优解，SGD是迂回式的趋于最优解，即在某个过程中，可能会向最优解相反方向迭代，另外，SGD方法最后得到的是最优解的近似值(BGD可以的到最优解)，但如果学习率选择不佳的化可能会存在无法找到最优解，甚至可能导致代价函数无法收敛；
Mini-Batch是前两者的折中方案，每次采用b个样本进行计算迭代，比BGD速度快，另外，如果将b个样本进行向量化处理，可以有效的利用并行方式进行速度优化，可能取得比SGD还要快的收敛速度，Mini-Batch的缺点是确定b需要消耗一部分时间精力

Thinking4	推荐系统中的冷启动都有哪些情况，有哪些常用的解决方法		

Thinking5	你阅读过和推荐系统，机器学习相关的论文么？有哪些论文是你比较推荐的，可以分享到微信群中		
